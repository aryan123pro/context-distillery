# Multi-Agent Context Compression Engine (MVP) — Plan

## Goal
Build an MVP “missing layer” that simulates a large context window by:
- running 3–5 cooperating agents (Orchestrator, 2–3 Workers, Compression, Retrieval)
- maintaining 3 memory tiers (STM, CWM, LTM)
- automatically compressing context and rehydrating only what is relevant
- measuring token savings (target ≥50%) and producing loss analysis
- supporting user-driven changes mid-task (overwriting/superseding prior decisions/constraints deterministically)

## Stack
- Backend: FastAPI (Python) + MongoDB (Motor)
- Frontend: React + Tailwind + shadcn/ui (minimal console)
- LLM: OpenAI via `emergentintegrations` (EMERGENT_LLM_KEY)

---

## Architecture

### Runtime objects
- **Run**: one long task, owns objective, configuration, state, and memory pointers.
- **Event Log**: every agent call and orchestrator action is logged deterministically with inputs/outputs.

### Agents (MVP)
1. **Orchestrator (non-LLM controller)**
   - controls step loop
   - decides when to compress
   - requests retrieval before any agent runs
   - merges worker outputs into a single “assistant reply” plus per-agent artifacts

2. **Worker Agents (LLM)**
   - **Reasoning/Planner Agent**: proposes next steps and concrete outputs
   - **Critic/Verifier Agent**: checks for consistency, missing constraints, and flags risk

3. **Compression Agent (LLM, critical)**
   - distills older STM + existing CWM into updated **structured memory**
   - must:
     - preserve variables/constraints/decisions
     - explicitly mark uncertainty
     - support **superseding** when user changes their mind

4. **Retrieval Agent (LLM + deterministic fallback)**
   - selects minimal relevant subset of memory for the next step
   - priority order for injection:
     1) constraints
     2) definitions
     3) decisions
     4) summaries/open loops

### Determinism modes
- **Best-effort deterministic (default)**: temperature=0, strict JSON schemas, logged prompts.
- **Strict deterministic fallback**: no LLM calls for compression/retrieval (rule-based extraction + keyword match). Useful for reproducibility and debugging.

---

## Memory Tiers (required)

### 1) STM (Short-Term Memory)
- Verbatim recent messages.
- Stored in Mongo as a message list per run.
- Windowed (e.g., last 12 messages).

### 2) CWM (Compressed Working Memory)
- Structured JSON generated by Compression Agent.

**Schema (MVP)**
```json
{
  "facts": [{"id":"...","text":"...","status":"active|deprecated","supersedes":[],"superseded_by":null,"confidence":"high|medium|low","source_message_ids":[]}],
  "decisions": [...],
  "constraints": [...],
  "assumptions": [...],
  "definitions": [{"term":"...","definition":"...", ...}],
  "open_loops": [{"id":"...","question":"...","owner":"orchestrator|planner|critic","status":"open|closed"}],
  "dropped": [{"text":"...","reason":"..."}],
  "updated_at": "ISO-8601"
}
```

**Overwrite / change handling**
- New constraints/decisions with the same “key/term” can supersede old ones.
- Old items are marked `deprecated` and linked via `superseded_by`.

### 3) LTM (Long-Term Memory)
- Stable, reusable items promoted by compression agent.
- Retrieved only when relevant.

### External storage
- MongoDB is source of truth.
- Hybrid inspectability: every compression cycle also writes a JSON snapshot to disk: `/app/backend/snapshots/{run_id}/{timestamp}.json`.

---

## Compression Triggers (required)
Compression runs when any is true:
- estimated token usage for assembled context exceeds threshold (configurable)
- step count hits periodic interval (e.g., every 4 steps)
- explicit `/compress` request
- phase change/handoff (MVP: detected via orchestrator state transitions)

---

## Context Rehydration (required)
Before any LLM agent runs:
1. Orchestrator calls Retrieval Agent with:
   - run objective
   - latest user message
   - current phase
   - current STM tail
   - CWM + LTM inventory (IDs + short previews)
2. Retrieval returns minimal subset (IDs).
3. Orchestrator injects memory in strict priority order.

---

## Token Usage + Evaluation Metrics (required)
We implement approximate token counting (deterministic) using character-based estimation:
- `est_tokens ≈ ceil(len(text)/4)`

Metrics per step:
- `baseline_tokens`: full transcript injection estimate
- `compressed_tokens`: rehydrated context injection estimate
- `reduction_pct`

Task success comparison (MVP):
- Optional “compare mode”: generate **baseline response** (full transcript) and **compressed response** (rehydrated context) and have Critic grade equivalence + note divergences.

Loss analysis:
- Compression agent produces `dropped[]` with explicit reasons.
- Critic flags any potentially harmful losses.

---

## Backend API (MVP)
All routes are prefixed with `/api`.

### Runs
- `POST /runs` — create a run
- `GET /runs/{run_id}` — run metadata + latest metrics
- `POST /runs/{run_id}/step` — submit user message, execute one orchestrated cycle
- `POST /runs/{run_id}/compress` — force compression

### Memory + Logs
- `GET /runs/{run_id}/memory` — STM tail + latest CWM + LTM
- `GET /runs/{run_id}/events` — agent-by-agent event log
- `GET /runs/{run_id}/snapshots/latest` — latest JSON snapshot (metadata + content)

### Demo
- `POST /demo/run` — creates a run and executes a scripted multi-step scenario (A or C) to demonstrate multiple compression cycles

---

## Frontend Flows (Minimal Web UI)
Single-page “Console”:
- Create run (objective, scenario A/C, thresholds, determinism mode)
- Chat input to send user messages
- Timeline showing:
  - orchestrator step
  - planner output
  - critic output
  - compression/retrieval events
- Memory viewer (Tabs: STM / CWM / LTM / Metrics)
- Demo runner button (auto-run scenario)

All interactive + critical info elements include `data-testid`.

---

## Testing Approach
- Backend: curl tests for run creation, stepping, compression, memory retrieval.
- Frontend: Playwright-based E2E via `testing_agent_v3`:
  - create run
  - send messages
  - confirm memory and metrics update
  - run demo scenario and observe multiple compressions
